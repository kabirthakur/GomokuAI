# Gomoku AI Project

## Authors

- Kabir Thakur – kathakur@syr.edu
- Kaustubh Desai – kndesai@syr.edu

## Methodology

To develop an AI capable of playing Gomoku, we implemented a combination of hard-coded moves and neural network-based heuristics. The AI's decision-making process is as follows:

1. **Winning Move Check**: Initially, the AI checks if it can win in the current turn by finding an open four-in-a-row.
2. **Opponent's Winning Move Check**: If no winning move is found, it checks if the opponent can win in their next move.
3. **Threat Management**: Depending on the situation, the AI then either:
   - Creates an open three position if no immediate threats are detected, using a move that minimizes the heuristic value from our neural network.
   - Blocks any serious threats identified, including opponent's potential winning moves or open threes, by selecting moves based on neural network heuristics.
4. **Heuristic-Based Move Selection**: If no immediate threats or opportunities are identified, the AI selects a move based on the neural network's heuristics.

## Neural Network

A dataset was generated by running 2000 Gomoku games and saving the outcomes. This dataset includes Game ID, move number, player (Min or Max), board state, winning player, final score, and a discounted score. The discounted score is calculated as follows:

$Discounted Score = Final Score × γ^(Total Moves - Move Number - 1)$


We experimented with different discount factors and settled on 0.9. The neural network was trained using PyTorch, focusing on games won by the Min player to avoid confusion from mixed reward signals in lost games. The network architecture consists of three convolutional layers and three fully connected layers, with a dropout rate of 0.3 to prevent overfitting. Mean Square Error was selected as the loss function. After experimentation, a learning rate of 0.001 was chosen.

## Discussion

Evaluating various versions of our AI indicated that the combination of hard-coded strategies and CNN-based heuristics allowed the AI to win or draw 60-70% of games, a significant improvement over using hard-coded strategies alone. Future improvements could include further hyperparameter tuning, experimenting with different network architectures, adjusting the discount factor, or refining the heuristic selection process. Integrating the AI with a Monte Carlo Tree Search Algorithm remains a challenging but promising area for future work.

## References

1. Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... & Hassabis, D. (2017). Mastering the game of go without human knowledge. Nature, 550(7676), 354-359.
2. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., … Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32 (pp. 8024–8035). Curran Associates, Inc. Retrieved from http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
3. Harris, C. R., Millman, K. J., Van Der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., ... & Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357-362. (Used in code)

